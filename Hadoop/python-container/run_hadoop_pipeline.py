# import subprocess

# MAPPER = "processing/mapper.py"
# REDUCER = "processing/reducer.py"
# HDFS_CSV = "/data/weapons.csv"
# OUTPUT_DIR = "/output_weapons"

# print("üìÑ Copie Mapper & Reducer dans le conteneur Hadoop...")
# subprocess.run(["docker", "cp", MAPPER, "hadoop-namenode:/mapper.py"])
# subprocess.run(["docker", "cp", REDUCER, "hadoop-namenode:/reducer.py"])
# print("‚úÖ Scripts MapReduce transf√©r√©s")

# print("‚öôÔ∏è  Ex√©cution du job MapReduce...")
# subprocess.run([
#     "docker", "exec", "hadoop-namenode", "bash", "-c",
#     f"hdfs dfs -rm -r {OUTPUT_DIR} || true && "
#     f"hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-*.jar "
#     f"-input {HDFS_CSV} -output {OUTPUT_DIR} -mapper /mapper.py -reducer /reducer.py"
# ])
# print("‚úÖ Job MapReduce termin√©")

# print("üì¶ R√©sultat du traitement :\n")
# subprocess.run([
#     "docker", "exec", "hadoop-namenode", "bash", "-c",
#     f"hdfs dfs -cat {OUTPUT_DIR}/part-00000"
# ])



# import subprocess

# MAPPER = "processing/mapper.py"
# REDUCER = "processing/reducer.py"
# HDFS_CSV = "/data/weapons.csv"
# OUTPUT_DIR = "/output_weapons"
# HADOOP_STREAMING_JAR = "/opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar"



# # --- Copier Mapper & Reducer ---
# print("üìÑ Copie Mapper & Reducer dans le conteneur Hadoop...")
# subprocess.run(["docker", "cp", MAPPER, "hadoop-namenode:/mapper.py"], check=True)
# subprocess.run(["docker", "cp", REDUCER, "hadoop-namenode:/reducer.py"], check=True)
# print("‚úÖ Scripts MapReduce transf√©r√©s")

# # --- Ex√©cuter le job MapReduce ---
# print("‚öôÔ∏è  Ex√©cution du job MapReduce...")
# subprocess.run([
#     "docker", "exec", "hadoop-namenode", "bash", "-c",
#     f"hdfs dfs -rm -r {OUTPUT_DIR} || true && "
#     f"hadoop jar {HADOOP_STREAMING_JAR} "
#     f"-input {HDFS_CSV} -output {OUTPUT_DIR} -mapper /mapper.py -reducer /reducer.py"
# ], check=True)
# print("‚úÖ Job MapReduce termin√©")

# # --- V√©rifier le r√©sultat ---
# print("üì¶ R√©sultat du traitement :")
# subprocess.run([
#     "docker", "exec", "hadoop-namenode", "hdfs", "dfs", "-cat", f"{OUTPUT_DIR}/part-00000"
# ])



import subprocess
import time
import os

# D√©termination des chemins r√©els
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

MAPPER_SRC = os.path.join(BASE_DIR, "processing", "mapper.py")
REDUCER_SRC = os.path.join(BASE_DIR, "processing", "reducer.py")
CSV_SRC = os.path.join(BASE_DIR, "..", "data", "weapons.csv")

print("üìÑ Copie Mapper & Reducer dans le conteneur Hadoop...")
subprocess.run([
    "docker", "cp", MAPPER_SRC, "hadoop-namenode:/mapper.py"
], check=True)
subprocess.run([
    "docker", "cp", REDUCER_SRC, "hadoop-namenode:/reducer.py"
], check=True)
print("‚úÖ Scripts MapReduce transf√©r√©s")

print("üìÑ Pr√©paration du dossier /data dans le conteneur...")
subprocess.run([
    "docker", "exec", "hadoop-namenode", "mkdir", "-p", "/data"
], check=True)
print("üìÅ Dossier /data cr√©√©")

print("üìÑ Copie du fichier CSV dans le conteneur Hadoop...")
subprocess.run([
    "docker", "cp", CSV_SRC, "hadoop-namenode:/data/weapons.csv"
], check=True)
print("‚úÖ CSV transf√©r√©")

print("üì§ Envoi du CSV dans HDFS...")
subprocess.run([
    "docker", "exec", "hadoop-namenode", "bash", "-c",
    "hdfs dfs -mkdir -p /data && hdfs dfs -put -f /data/weapons.csv /data/weapons.csv"
], check=True)
print("‚úÖ CSV charg√© dans HDFS")

print("‚öôÔ∏è  Ex√©cution du job MapReduce...")
subprocess.run([
    "docker", "exec", "hadoop-namenode", "bash", "-c",
    "hdfs dfs -rm -r /output_weapons || true && "
    "hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar "
    "-input /data/weapons.csv -output /output_weapons "
    "-mapper /mapper.py -reducer /reducer.py"
], check=True)

print("üéâ Job MapReduce termin√© !")

