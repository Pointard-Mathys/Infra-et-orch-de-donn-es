# ---
# - name: DÃ©ploiement Big Data sÃ©parÃ©
#   hosts: localhost
#   vars:
#     repo_dest_kafka: "/home/mathys/Infra-et-orch-de-donn-es"
#     repo_dest_hadoop: "/home/mathys/Infra-et-orch-de-donn-es/Hadoop"
#     repo_dest_hive: "/home/mathys/Infra-et-orch-de-donn-es/Hive"

#   tasks:
#     - name: Ensure Docker network exists
#       community.docker.docker_network:
#         name: infra-net
#         state: present

#     - name: ğŸ§± DÃ©marrer stack Kafka
#       command: docker compose up -d --build
#       args:
#         chdir: "{{ repo_dest_kafka }}"

#     - name: ğŸ§± DÃ©marrer stack Hadoop
#       command: docker compose up -d --build
#       args:
#         chdir: "{{ repo_dest_hadoop }}"

#     - name: â³ Attendre que HDFS soit prÃªt
#       shell: |
#         until docker exec hadoop-namenode hdfs dfs -ls /; do sleep 5; done
#       retries: 20
#       delay: 5

#     - name: ğŸ§± DÃ©marrer stack Hive
#       command: docker compose up -d --build
#       args:
#         chdir: "{{ repo_dest_hive }}"

#     - name: â³ Attendre que Hive soit prÃªt
#       shell: |
#         until docker exec hadoop-hive hive -e 'SHOW DATABASES;' >/dev/null 2>&1; do sleep 5; done
#       retries: 20
#       delay: 5

#     - name: âœ… VÃ©rification Hive
#       shell: docker exec hadoop-hive hive -e 'SHOW DATABASES;'







---
- name: DÃ©ploiement complet Big Data avec Hive
  hosts: localhost
  vars:
    repo_dest_kafka: "/home/mathys/Infra-et-orch-de-donn-es"
    repo_dest_hadoop: "/home/mathys/Infra-et-orch-de-donn-es/Hadoop"
    repo_dest_scripts: "/home/mathys/Infra-et-orch-de-donn-es/Hadoop/python-container"
    repo_dest_data: "/home/mathys/Infra-et-orch-de-donn-es/Hadoop/data"
    hadoop_container: "hadoop-namenode"

  tasks:
    - name: Ensure Docker network infra-net exists
      community.docker.docker_network:
        name: infra-net
        state: present

    - name: ğŸ§± DÃ©marrer stack Kafka + PostgreSQL
      command: docker compose up -d --build
      args:
        chdir: "{{ repo_dest_kafka }}"

    - name: ğŸŒ DÃ©marrer stack Hadoop + Hive
      command: docker compose up -d --build
      args:
        chdir: "{{ repo_dest_hadoop }}"

    - name: â³ Attendre que HDFS soit prÃªt
      shell: |
        until docker exec {{ hadoop_container }} hdfs dfs -ls / >/dev/null 2>&1; do sleep 5; done
      retries: 20
      delay: 5

    # --- Copier Mapper, Reducer et CSV dans le conteneur si prÃ©sents ---
    - name: Copier mapper.py dans le conteneur Hadoop
      command: docker cp {{ repo_dest_scripts }}/mapper.py {{ hadoop_container }}:/mapper.py
      ignore_errors: yes

    - name: Copier reducer.py dans le conteneur Hadoop
      command: docker cp {{ repo_dest_scripts }}/reducer.py {{ hadoop_container }}:/reducer.py
      ignore_errors: yes

    - name: Copier weapons.csv dans le conteneur Hadoop
      command: docker cp {{ repo_dest_data }}/weapons.csv {{ hadoop_container }}:/weapons.csv
      ignore_errors: yes

    - name: Copier run_hadoop_pipeline.sh dans le conteneur Hadoop
      command: docker cp {{ repo_dest_scripts }}/run_hadoop_pipeline.sh {{ hadoop_container }}:/run_hadoop_pipeline.sh
      ignore_errors: yes

    - name: Donner les droits d'exÃ©cution aux scripts prÃ©sents
      command: >
        docker exec {{ hadoop_container }} sh -c "
        for f in /mapper.py /reducer.py /run_hadoop_pipeline.sh; do
          if [ -f \$f ]; then chmod +x \$f; fi
        done || true"
      ignore_errors: yes

    # --- Lancer le pipeline Hadoop automatiquement si le script existe ---
    - name: âš™ï¸ Lancer job Hadoop avec run_hadoop_pipeline.sh
      command: docker exec {{ hadoop_container }} bash -c "[ -f /run_hadoop_pipeline.sh ] && /run_hadoop_pipeline.sh || echo 'run_hadoop_pipeline.sh absent, skipping'"

    # --- âš¡ ExÃ©cuter le fake pipeline directement ---
    - name: âš¡ ExÃ©cuter le pipeline fake MapReduce + Hive
      command: python3 fake_pipeline.py
      args:
        chdir: "{{ repo_dest_scripts }}"
      register: pipeline_output
      ignore_errors: yes

    - name: Afficher le rÃ©sultat du pipeline fake
      debug:
        var: pipeline_output.stdout_lines

    # --- Optionnel : exÃ©cuter insert_to_hive.py classique (peut Ãªtre ignorÃ© si fake suffisant) ---
    # - name: âš™ï¸ Charger rÃ©sultats dans Hive
    #   command: python3 insert_to_hive.py
    #   args:
    #     chdir: "{{ repo_dest_scripts }}"
    #   ignore_errors: yes



    - name: ğŸ“ˆ DÃ©marrer Prometheus + Grafana
      command: docker compose up -d
      args:
        chdir: "{{ repo_dest_kafka }}/monitoring"


    # - name: ğŸ“ˆ DÃ©marrer Prometheus + Grafana
    #   command: docker compose up -d --build
    #   args:
    #     chdir: "/home/mathys/Infra-et-orch-de-donn-es/monitoring"
